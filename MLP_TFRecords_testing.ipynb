{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tpu.python.tpu import tpu_config\n",
    "from tensorflow.contrib.tpu.python.tpu import tpu_estimator\n",
    "from tensorflow.contrib.tpu.python.tpu import tpu_optimizer\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_DIR = './MLP/model_2'\n",
    "DATA_DIR = './MLP_data'\n",
    "\n",
    "prefetch_buffer_size = 128\n",
    "num_files_infeed = 16\n",
    "shuffle_buffer_size = 512\n",
    "num_parallel_calls = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern = os.path.join(\n",
    "        DATA_DIR, 'MLP_data_train*')\n",
    "\n",
    "dataset = tf.data.Dataset.list_files(file_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ShuffleDataset shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./MLP_data/MLP_data_train-1.tfrecords\t ./MLP_data/MLP_data_train-41.tfrecords\r\n",
      "./MLP_data/MLP_data_train-10.tfrecords\t ./MLP_data/MLP_data_train-42.tfrecords\r\n",
      "./MLP_data/MLP_data_train-100.tfrecords  ./MLP_data/MLP_data_train-43.tfrecords\r\n",
      "./MLP_data/MLP_data_train-101.tfrecords  ./MLP_data/MLP_data_train-44.tfrecords\r\n",
      "./MLP_data/MLP_data_train-102.tfrecords  ./MLP_data/MLP_data_train-45.tfrecords\r\n",
      "./MLP_data/MLP_data_train-103.tfrecords  ./MLP_data/MLP_data_train-46.tfrecords\r\n",
      "./MLP_data/MLP_data_train-104.tfrecords  ./MLP_data/MLP_data_train-47.tfrecords\r\n",
      "./MLP_data/MLP_data_train-105.tfrecords  ./MLP_data/MLP_data_train-48.tfrecords\r\n",
      "./MLP_data/MLP_data_train-106.tfrecords  ./MLP_data/MLP_data_train-49.tfrecords\r\n",
      "./MLP_data/MLP_data_train-107.tfrecords  ./MLP_data/MLP_data_train-5.tfrecords\r\n",
      "./MLP_data/MLP_data_train-108.tfrecords  ./MLP_data/MLP_data_train-50.tfrecords\r\n",
      "./MLP_data/MLP_data_train-109.tfrecords  ./MLP_data/MLP_data_train-51.tfrecords\r\n",
      "./MLP_data/MLP_data_train-11.tfrecords\t ./MLP_data/MLP_data_train-52.tfrecords\r\n",
      "./MLP_data/MLP_data_train-110.tfrecords  ./MLP_data/MLP_data_train-53.tfrecords\r\n",
      "./MLP_data/MLP_data_train-111.tfrecords  ./MLP_data/MLP_data_train-54.tfrecords\r\n",
      "./MLP_data/MLP_data_train-112.tfrecords  ./MLP_data/MLP_data_train-55.tfrecords\r\n",
      "./MLP_data/MLP_data_train-113.tfrecords  ./MLP_data/MLP_data_train-56.tfrecords\r\n",
      "./MLP_data/MLP_data_train-114.tfrecords  ./MLP_data/MLP_data_train-57.tfrecords\r\n",
      "./MLP_data/MLP_data_train-115.tfrecords  ./MLP_data/MLP_data_train-58.tfrecords\r\n",
      "./MLP_data/MLP_data_train-116.tfrecords  ./MLP_data/MLP_data_train-59.tfrecords\r\n",
      "./MLP_data/MLP_data_train-117.tfrecords  ./MLP_data/MLP_data_train-6.tfrecords\r\n",
      "./MLP_data/MLP_data_train-118.tfrecords  ./MLP_data/MLP_data_train-60.tfrecords\r\n",
      "./MLP_data/MLP_data_train-119.tfrecords  ./MLP_data/MLP_data_train-61.tfrecords\r\n",
      "./MLP_data/MLP_data_train-12.tfrecords\t ./MLP_data/MLP_data_train-62.tfrecords\r\n",
      "./MLP_data/MLP_data_train-120.tfrecords  ./MLP_data/MLP_data_train-63.tfrecords\r\n",
      "./MLP_data/MLP_data_train-121.tfrecords  ./MLP_data/MLP_data_train-64.tfrecords\r\n",
      "./MLP_data/MLP_data_train-122.tfrecords  ./MLP_data/MLP_data_train-65.tfrecords\r\n",
      "./MLP_data/MLP_data_train-123.tfrecords  ./MLP_data/MLP_data_train-66.tfrecords\r\n",
      "./MLP_data/MLP_data_train-124.tfrecords  ./MLP_data/MLP_data_train-67.tfrecords\r\n",
      "./MLP_data/MLP_data_train-125.tfrecords  ./MLP_data/MLP_data_train-68.tfrecords\r\n",
      "./MLP_data/MLP_data_train-126.tfrecords  ./MLP_data/MLP_data_train-69.tfrecords\r\n",
      "./MLP_data/MLP_data_train-127.tfrecords  ./MLP_data/MLP_data_train-7.tfrecords\r\n",
      "./MLP_data/MLP_data_train-128.tfrecords  ./MLP_data/MLP_data_train-70.tfrecords\r\n",
      "./MLP_data/MLP_data_train-13.tfrecords\t ./MLP_data/MLP_data_train-71.tfrecords\r\n",
      "./MLP_data/MLP_data_train-14.tfrecords\t ./MLP_data/MLP_data_train-72.tfrecords\r\n",
      "./MLP_data/MLP_data_train-15.tfrecords\t ./MLP_data/MLP_data_train-73.tfrecords\r\n",
      "./MLP_data/MLP_data_train-16.tfrecords\t ./MLP_data/MLP_data_train-74.tfrecords\r\n",
      "./MLP_data/MLP_data_train-17.tfrecords\t ./MLP_data/MLP_data_train-75.tfrecords\r\n",
      "./MLP_data/MLP_data_train-18.tfrecords\t ./MLP_data/MLP_data_train-76.tfrecords\r\n",
      "./MLP_data/MLP_data_train-19.tfrecords\t ./MLP_data/MLP_data_train-77.tfrecords\r\n",
      "./MLP_data/MLP_data_train-2.tfrecords\t ./MLP_data/MLP_data_train-78.tfrecords\r\n",
      "./MLP_data/MLP_data_train-20.tfrecords\t ./MLP_data/MLP_data_train-79.tfrecords\r\n",
      "./MLP_data/MLP_data_train-21.tfrecords\t ./MLP_data/MLP_data_train-8.tfrecords\r\n",
      "./MLP_data/MLP_data_train-22.tfrecords\t ./MLP_data/MLP_data_train-80.tfrecords\r\n",
      "./MLP_data/MLP_data_train-23.tfrecords\t ./MLP_data/MLP_data_train-81.tfrecords\r\n",
      "./MLP_data/MLP_data_train-24.tfrecords\t ./MLP_data/MLP_data_train-82.tfrecords\r\n",
      "./MLP_data/MLP_data_train-25.tfrecords\t ./MLP_data/MLP_data_train-83.tfrecords\r\n",
      "./MLP_data/MLP_data_train-26.tfrecords\t ./MLP_data/MLP_data_train-84.tfrecords\r\n",
      "./MLP_data/MLP_data_train-27.tfrecords\t ./MLP_data/MLP_data_train-85.tfrecords\r\n",
      "./MLP_data/MLP_data_train-28.tfrecords\t ./MLP_data/MLP_data_train-86.tfrecords\r\n",
      "./MLP_data/MLP_data_train-29.tfrecords\t ./MLP_data/MLP_data_train-87.tfrecords\r\n",
      "./MLP_data/MLP_data_train-3.tfrecords\t ./MLP_data/MLP_data_train-88.tfrecords\r\n",
      "./MLP_data/MLP_data_train-30.tfrecords\t ./MLP_data/MLP_data_train-89.tfrecords\r\n",
      "./MLP_data/MLP_data_train-31.tfrecords\t ./MLP_data/MLP_data_train-9.tfrecords\r\n",
      "./MLP_data/MLP_data_train-32.tfrecords\t ./MLP_data/MLP_data_train-90.tfrecords\r\n",
      "./MLP_data/MLP_data_train-33.tfrecords\t ./MLP_data/MLP_data_train-91.tfrecords\r\n",
      "./MLP_data/MLP_data_train-34.tfrecords\t ./MLP_data/MLP_data_train-92.tfrecords\r\n",
      "./MLP_data/MLP_data_train-35.tfrecords\t ./MLP_data/MLP_data_train-93.tfrecords\r\n",
      "./MLP_data/MLP_data_train-36.tfrecords\t ./MLP_data/MLP_data_train-94.tfrecords\r\n",
      "./MLP_data/MLP_data_train-37.tfrecords\t ./MLP_data/MLP_data_train-95.tfrecords\r\n",
      "./MLP_data/MLP_data_train-38.tfrecords\t ./MLP_data/MLP_data_train-96.tfrecords\r\n",
      "./MLP_data/MLP_data_train-39.tfrecords\t ./MLP_data/MLP_data_train-97.tfrecords\r\n",
      "./MLP_data/MLP_data_train-4.tfrecords\t ./MLP_data/MLP_data_train-98.tfrecords\r\n",
      "./MLP_data/MLP_data_train-40.tfrecords\t ./MLP_data/MLP_data_train-99.tfrecords\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./MLP_data/MLP_data_train*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./MLP_data/MLP_data_train-1.tfrecords\t ./MLP_data/MLP_data_train-41.tfrecords\r\n",
      "./MLP_data/MLP_data_train-10.tfrecords\t ./MLP_data/MLP_data_train-42.tfrecords\r\n",
      "./MLP_data/MLP_data_train-100.tfrecords  ./MLP_data/MLP_data_train-43.tfrecords\r\n",
      "./MLP_data/MLP_data_train-101.tfrecords  ./MLP_data/MLP_data_train-44.tfrecords\r\n",
      "./MLP_data/MLP_data_train-102.tfrecords  ./MLP_data/MLP_data_train-45.tfrecords\r\n",
      "./MLP_data/MLP_data_train-103.tfrecords  ./MLP_data/MLP_data_train-46.tfrecords\r\n",
      "./MLP_data/MLP_data_train-104.tfrecords  ./MLP_data/MLP_data_train-47.tfrecords\r\n",
      "./MLP_data/MLP_data_train-105.tfrecords  ./MLP_data/MLP_data_train-48.tfrecords\r\n",
      "./MLP_data/MLP_data_train-106.tfrecords  ./MLP_data/MLP_data_train-49.tfrecords\r\n",
      "./MLP_data/MLP_data_train-107.tfrecords  ./MLP_data/MLP_data_train-5.tfrecords\r\n",
      "./MLP_data/MLP_data_train-108.tfrecords  ./MLP_data/MLP_data_train-50.tfrecords\r\n",
      "./MLP_data/MLP_data_train-109.tfrecords  ./MLP_data/MLP_data_train-51.tfrecords\r\n",
      "./MLP_data/MLP_data_train-11.tfrecords\t ./MLP_data/MLP_data_train-52.tfrecords\r\n",
      "./MLP_data/MLP_data_train-110.tfrecords  ./MLP_data/MLP_data_train-53.tfrecords\r\n",
      "./MLP_data/MLP_data_train-111.tfrecords  ./MLP_data/MLP_data_train-54.tfrecords\r\n",
      "./MLP_data/MLP_data_train-112.tfrecords  ./MLP_data/MLP_data_train-55.tfrecords\r\n",
      "./MLP_data/MLP_data_train-113.tfrecords  ./MLP_data/MLP_data_train-56.tfrecords\r\n",
      "./MLP_data/MLP_data_train-114.tfrecords  ./MLP_data/MLP_data_train-57.tfrecords\r\n",
      "./MLP_data/MLP_data_train-115.tfrecords  ./MLP_data/MLP_data_train-58.tfrecords\r\n",
      "./MLP_data/MLP_data_train-116.tfrecords  ./MLP_data/MLP_data_train-59.tfrecords\r\n",
      "./MLP_data/MLP_data_train-117.tfrecords  ./MLP_data/MLP_data_train-6.tfrecords\r\n",
      "./MLP_data/MLP_data_train-118.tfrecords  ./MLP_data/MLP_data_train-60.tfrecords\r\n",
      "./MLP_data/MLP_data_train-119.tfrecords  ./MLP_data/MLP_data_train-61.tfrecords\r\n",
      "./MLP_data/MLP_data_train-12.tfrecords\t ./MLP_data/MLP_data_train-62.tfrecords\r\n",
      "./MLP_data/MLP_data_train-120.tfrecords  ./MLP_data/MLP_data_train-63.tfrecords\r\n",
      "./MLP_data/MLP_data_train-121.tfrecords  ./MLP_data/MLP_data_train-64.tfrecords\r\n",
      "./MLP_data/MLP_data_train-122.tfrecords  ./MLP_data/MLP_data_train-65.tfrecords\r\n",
      "./MLP_data/MLP_data_train-123.tfrecords  ./MLP_data/MLP_data_train-66.tfrecords\r\n",
      "./MLP_data/MLP_data_train-124.tfrecords  ./MLP_data/MLP_data_train-67.tfrecords\r\n",
      "./MLP_data/MLP_data_train-125.tfrecords  ./MLP_data/MLP_data_train-68.tfrecords\r\n",
      "./MLP_data/MLP_data_train-126.tfrecords  ./MLP_data/MLP_data_train-69.tfrecords\r\n",
      "./MLP_data/MLP_data_train-127.tfrecords  ./MLP_data/MLP_data_train-7.tfrecords\r\n",
      "./MLP_data/MLP_data_train-128.tfrecords  ./MLP_data/MLP_data_train-70.tfrecords\r\n",
      "./MLP_data/MLP_data_train-13.tfrecords\t ./MLP_data/MLP_data_train-71.tfrecords\r\n",
      "./MLP_data/MLP_data_train-14.tfrecords\t ./MLP_data/MLP_data_train-72.tfrecords\r\n",
      "./MLP_data/MLP_data_train-15.tfrecords\t ./MLP_data/MLP_data_train-73.tfrecords\r\n",
      "./MLP_data/MLP_data_train-16.tfrecords\t ./MLP_data/MLP_data_train-74.tfrecords\r\n",
      "./MLP_data/MLP_data_train-17.tfrecords\t ./MLP_data/MLP_data_train-75.tfrecords\r\n",
      "./MLP_data/MLP_data_train-18.tfrecords\t ./MLP_data/MLP_data_train-76.tfrecords\r\n",
      "./MLP_data/MLP_data_train-19.tfrecords\t ./MLP_data/MLP_data_train-77.tfrecords\r\n",
      "./MLP_data/MLP_data_train-2.tfrecords\t ./MLP_data/MLP_data_train-78.tfrecords\r\n",
      "./MLP_data/MLP_data_train-20.tfrecords\t ./MLP_data/MLP_data_train-79.tfrecords\r\n",
      "./MLP_data/MLP_data_train-21.tfrecords\t ./MLP_data/MLP_data_train-8.tfrecords\r\n",
      "./MLP_data/MLP_data_train-22.tfrecords\t ./MLP_data/MLP_data_train-80.tfrecords\r\n",
      "./MLP_data/MLP_data_train-23.tfrecords\t ./MLP_data/MLP_data_train-81.tfrecords\r\n",
      "./MLP_data/MLP_data_train-24.tfrecords\t ./MLP_data/MLP_data_train-82.tfrecords\r\n",
      "./MLP_data/MLP_data_train-25.tfrecords\t ./MLP_data/MLP_data_train-83.tfrecords\r\n",
      "./MLP_data/MLP_data_train-26.tfrecords\t ./MLP_data/MLP_data_train-84.tfrecords\r\n",
      "./MLP_data/MLP_data_train-27.tfrecords\t ./MLP_data/MLP_data_train-85.tfrecords\r\n",
      "./MLP_data/MLP_data_train-28.tfrecords\t ./MLP_data/MLP_data_train-86.tfrecords\r\n",
      "./MLP_data/MLP_data_train-29.tfrecords\t ./MLP_data/MLP_data_train-87.tfrecords\r\n",
      "./MLP_data/MLP_data_train-3.tfrecords\t ./MLP_data/MLP_data_train-88.tfrecords\r\n",
      "./MLP_data/MLP_data_train-30.tfrecords\t ./MLP_data/MLP_data_train-89.tfrecords\r\n",
      "./MLP_data/MLP_data_train-31.tfrecords\t ./MLP_data/MLP_data_train-9.tfrecords\r\n",
      "./MLP_data/MLP_data_train-32.tfrecords\t ./MLP_data/MLP_data_train-90.tfrecords\r\n",
      "./MLP_data/MLP_data_train-33.tfrecords\t ./MLP_data/MLP_data_train-91.tfrecords\r\n",
      "./MLP_data/MLP_data_train-34.tfrecords\t ./MLP_data/MLP_data_train-92.tfrecords\r\n",
      "./MLP_data/MLP_data_train-35.tfrecords\t ./MLP_data/MLP_data_train-93.tfrecords\r\n",
      "./MLP_data/MLP_data_train-36.tfrecords\t ./MLP_data/MLP_data_train-94.tfrecords\r\n",
      "./MLP_data/MLP_data_train-37.tfrecords\t ./MLP_data/MLP_data_train-95.tfrecords\r\n",
      "./MLP_data/MLP_data_train-38.tfrecords\t ./MLP_data/MLP_data_train-96.tfrecords\r\n",
      "./MLP_data/MLP_data_train-39.tfrecords\t ./MLP_data/MLP_data_train-97.tfrecords\r\n",
      "./MLP_data/MLP_data_train-4.tfrecords\t ./MLP_data/MLP_data_train-98.tfrecords\r\n",
      "./MLP_data/MLP_data_train-40.tfrecords\t ./MLP_data/MLP_data_train-99.tfrecords\r\n"
     ]
    }
   ],
   "source": [
    "!ls $file_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Input(object):\n",
    "  \"\"\"Wrapper class that acts as the input_fn to TPUEstimator.\"\"\"\n",
    "\n",
    "  def __init__(self, is_training=True, is_eval=True, data_dir=None):\n",
    "    self.is_eval = is_eval\n",
    "    self.is_training = is_training\n",
    "    self.data_dir = data_dir if data_dir else DATA_DIR\n",
    "\n",
    "  def dataset_parser(self, value):\n",
    "    \"\"\"Parse an Imagenet record from value.\"\"\"\n",
    "    keys_to_features = {\n",
    "        'X': tf.FixedLenFeature([], dtype=tf.string),\n",
    "        'y': tf.FixedLenFeature(shape=[1], dtype=tf.int64)            \n",
    "    }\n",
    "    parsed = tf.parse_single_example(value, keys_to_features)\n",
    "    X = tf.decode_raw(parsed['X'], tf.float32)\n",
    "    X = tf.reshape(X, [10000])\n",
    "    \n",
    "    y = tf.cast(parsed['y'], tf.int64)\n",
    "    return X, y\n",
    "\n",
    "  def __call__(self, params):\n",
    "    \"\"\"Input function which provides a single batch for train or eval.\"\"\"\n",
    "    # Retrieves the batch size for the current shard. The # of shards is\n",
    "    # computed according to the input pipeline deployment. See\n",
    "    # `tf.contrib.tpu.RunConfig` for details.\n",
    "    batch_size = params['batch_size']\n",
    "\n",
    "    # Shuffle the filenames to ensure better randomization\n",
    "    file_pattern = os.path.join(\n",
    "        self.data_dir, 'MLP_data_train*' if self.is_training \n",
    "        else 'MLP_data_test*' )\n",
    "    dataset = tf.data.Dataset.list_files(file_pattern)\n",
    "    \n",
    "    #pdb.set_trace()\n",
    "    \n",
    "    if self.is_training:\n",
    "      dataset = dataset.shuffle(buffer_size=128)  # 1024 files in dataset\n",
    "\n",
    "    if self.is_training:\n",
    "      dataset = dataset.repeat()\n",
    "\n",
    "    def prefetch_dataset(filename):\n",
    "      buffer_size =  prefetch_buffer_size\n",
    "      dataset = tf.data.TFRecordDataset(filename, buffer_size=buffer_size)\n",
    "      return dataset\n",
    "\n",
    "    dataset = dataset.apply(\n",
    "        tf.contrib.data.parallel_interleave(\n",
    "            prefetch_dataset, cycle_length= num_files_infeed,\n",
    "            sloppy=True))\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        self.dataset_parser,\n",
    "        num_parallel_calls=num_parallel_calls)\n",
    "    dataset = dataset.prefetch(batch_size)\n",
    "    dataset = dataset.apply(\n",
    "        tf.contrib.data.batch_and_drop_remainder(batch_size))\n",
    "\n",
    "    dataset = dataset.prefetch(2)  # Prefetch overlaps in-feed with training\n",
    "    images, labels = dataset.make_one_shot_iterator().get_next()\n",
    "    \n",
    "    if self.is_training or self.is_eval:\n",
    "          return images, labels\n",
    "    else:\n",
    "          return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = MLP_Input(is_training=False)\n",
    "X, y = my_data({'batch_size':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "  valX, valy = sess.run([X, y])\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10000) [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "(10, 10000) [[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "(10, 10000) [[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "(10, 10000) [[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "(10, 10000) [[1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "data_path = './MLP_data/MLP_data_test.tfrecords'  # address to save the hdf5 file\n",
    "with tf.Session() as sess:\n",
    "    feature =  {\n",
    "        'X': tf.FixedLenFeature([], dtype=tf.string),\n",
    "        'y': tf.FixedLenFeature(shape=[1], dtype=tf.int64)            \n",
    "    }\n",
    "    # Create a list of filenames and pass it to a queue\n",
    "    filename_queue = tf.train.string_input_producer([data_path], num_epochs=1)\n",
    "    # Define a reader and read the next record\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    # Decode the record read by the reader\n",
    "    features = tf.parse_single_example(serialized_example, features=feature)\n",
    "    # Convert the image data from string back to the numbers\n",
    "    image = tf.decode_raw(features['X'], tf.float32)\n",
    "    \n",
    "    # Cast label data into int32\n",
    "    label = tf.cast(features['y'], tf.int32)\n",
    "    # Reshape image data into the original shape\n",
    "    image = tf.reshape(image, [10000])\n",
    "    \n",
    "    # Any preprocessing here ...\n",
    "    \n",
    "    # Creates batches by randomly shuffling tensors\n",
    "    images, labels = tf.train.shuffle_batch([image, label], batch_size=10, capacity=30, num_threads=1, min_after_dequeue=10)\n",
    "    \n",
    "        # Initialize all global and local variables\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    # Create a coordinator and run all QueueRunner objects\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    for batch_index in range(5):\n",
    "        img, lbl = sess.run([images, labels])\n",
    "        print(img.shape, lbl)\n",
    "    # Stop the threads\n",
    "    coord.request_stop()\n",
    "    \n",
    "    # Wait for threads to stop\n",
    "    coord.join(threads)\n",
    "    sess.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.33810377,  1.39111209, -0.29905799, ..., -1.40061259,\n",
       "        -0.09094267, -0.45051607],\n",
       "       [-0.15256213,  0.07564683,  0.37871063, ..., -0.32956102,\n",
       "         1.00066197, -0.14797863],\n",
       "       [ 1.21993184,  1.79746187, -0.94660622, ...,  0.46586004,\n",
       "        -0.00414874,  1.08658051],\n",
       "       ..., \n",
       "       [ 1.32165027, -0.18541497,  1.36093473, ..., -1.13356423,\n",
       "        -0.94628513,  0.81919318],\n",
       "       [-0.6391685 , -1.20876503, -0.11585992, ...,  1.19735825,\n",
       "        -1.26576638,  0.20602697],\n",
       "       [-0.09409259,  0.17335817,  0.55533248, ..., -0.64466059,\n",
       "         1.07462323,  0.84672332]], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
